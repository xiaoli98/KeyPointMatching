{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize as tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from track_1_kp_matching import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data: 20635\n",
      "length of train labels: 20635\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\super\\Desktop\\HLT_Project\\KeyPointMatching\\main.ipynb Cella 2\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/super/Desktop/HLT_Project/KeyPointMatching/main.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataPreprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/super/Desktop/HLT_Project/KeyPointMatching/main.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m Data()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/super/Desktop/HLT_Project/KeyPointMatching/main.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data\u001b[39m.\u001b[39;49mget_data_from(path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkpm_data\u001b[39;49m\u001b[39m\"\u001b[39;49m,subset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\super\\Desktop\\HLT_Project\\KeyPointMatching\\dataPreprocess.py:274\u001b[0m, in \u001b[0;36mData.get_data_from\u001b[1;34m(self, path, subset)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlength of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m labels: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subset, \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(targets))))\n\u001b[0;32m    273\u001b[0m tokenized_data \u001b[39m=\u001b[39m tokenizer(data, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 274\u001b[0m \u001b[39mprint\u001b[39m(tokenized_data[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    275\u001b[0m \u001b[39minput\u001b[39m()\n\u001b[0;32m    276\u001b[0m tokenized_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((\u001b[39mdict\u001b[39m(tokenized_data), targets))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings[item]\n\u001b[0;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndexing with integers (to access backend Encoding for a given batch index) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not available when using Python based tokenizers\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'"
     ]
    }
   ],
   "source": [
    "from dataPreprocess import * \n",
    "data = Data()\n",
    "data.get_data_from(path=\"kpm_data\",subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ֿ** loading task data:\n",
      "\t(-1, 'Assisted suicide should be a criminal offence'): loaded 121 arguments and 4 key points\n",
      "\t(-1, 'Homeschooling should be banned'): loaded 129 arguments and 6 key points\n",
      "\t(-1, 'The vow of celibacy should be abandoned'): loaded 112 arguments and 6 key points\n",
      "\t(-1, 'We should abandon marriage'): loaded 111 arguments and 5 key points\n",
      "\t(-1, 'We should abolish capital punishment'): loaded 110 arguments and 5 key points\n",
      "\t(-1, 'We should abolish intellectual property rights'): loaded 123 arguments and 4 key points\n",
      "\t(-1, 'We should adopt atheism'): loaded 123 arguments and 3 key points\n",
      "\t(-1, 'We should adopt libertarianism'): loaded 113 arguments and 5 key points\n",
      "\t(-1, 'We should ban human cloning'): loaded 123 arguments and 5 key points\n",
      "\t(-1, 'We should ban private military companies'): loaded 106 arguments and 4 key points\n",
      "\t(-1, 'We should ban the use of child actors'): loaded 121 arguments and 5 key points\n",
      "\t(-1, 'We should close Guantanamo Bay detention camp'): loaded 113 arguments and 3 key points\n",
      "\t(-1, 'We should end mandatory retirement'): loaded 78 arguments and 2 key points\n",
      "\t(-1, 'We should fight for the abolition of nuclear weapons'): loaded 80 arguments and 1 key points\n",
      "\t(-1, 'We should fight urbanization'): loaded 106 arguments and 3 key points\n",
      "\t(-1, 'We should introduce compulsory voting'): loaded 129 arguments and 4 key points\n",
      "\t(-1, 'We should legalize cannabis'): loaded 109 arguments and 5 key points\n",
      "\t(-1, 'We should legalize prostitution'): loaded 99 arguments and 4 key points\n",
      "\t(-1, 'We should legalize sex selection'): loaded 126 arguments and 6 key points\n",
      "\t(-1, 'We should prohibit flag burning'): loaded 118 arguments and 2 key points\n",
      "\t(-1, 'We should prohibit women in combat'): loaded 130 arguments and 6 key points\n",
      "\t(-1, 'We should subsidize journalism'): loaded 110 arguments and 3 key points\n",
      "\t(-1, 'We should subsidize space exploration'): loaded 115 arguments and 4 key points\n",
      "\t(-1, 'We should subsidize vocational education'): loaded 95 arguments and 4 key points\n",
      "\t(1, 'Assisted suicide should be a criminal offence'): loaded 125 arguments and 6 key points\n",
      "\t(1, 'Homeschooling should be banned'): loaded 115 arguments and 4 key points\n",
      "\t(1, 'The vow of celibacy should be abandoned'): loaded 122 arguments and 5 key points\n",
      "\t(1, 'We should abandon marriage'): loaded 125 arguments and 4 key points\n",
      "\t(1, 'We should abolish capital punishment'): loaded 126 arguments and 5 key points\n",
      "\t(1, 'We should abolish intellectual property rights'): loaded 91 arguments and 6 key points\n",
      "\t(1, 'We should adopt atheism'): loaded 105 arguments and 4 key points\n",
      "\t(1, 'We should adopt libertarianism'): loaded 124 arguments and 4 key points\n",
      "\t(1, 'We should ban human cloning'): loaded 122 arguments and 4 key points\n",
      "\t(1, 'We should ban private military companies'): loaded 126 arguments and 5 key points\n",
      "\t(1, 'We should ban the use of child actors'): loaded 123 arguments and 5 key points\n",
      "\t(1, 'We should close Guantanamo Bay detention camp'): loaded 128 arguments and 6 key points\n",
      "\t(1, 'We should end mandatory retirement'): loaded 122 arguments and 5 key points\n",
      "\t(1, 'We should fight for the abolition of nuclear weapons'): loaded 116 arguments and 2 key points\n",
      "\t(1, 'We should fight urbanization'): loaded 127 arguments and 4 key points\n",
      "\t(1, 'We should introduce compulsory voting'): loaded 115 arguments and 3 key points\n",
      "\t(1, 'We should legalize cannabis'): loaded 137 arguments and 6 key points\n",
      "\t(1, 'We should legalize prostitution'): loaded 132 arguments and 5 key points\n",
      "\t(1, 'We should legalize sex selection'): loaded 118 arguments and 5 key points\n",
      "\t(1, 'We should prohibit flag burning'): loaded 99 arguments and 2 key points\n",
      "\t(1, 'We should prohibit women in combat'): loaded 107 arguments and 5 key points\n",
      "\t(1, 'We should subsidize journalism'): loaded 118 arguments and 4 key points\n",
      "\t(1, 'We should subsidize space exploration'): loaded 132 arguments and 5 key points\n",
      "\t(1, 'We should subsidize vocational education'): loaded 128 arguments and 4 key points\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\n",
      "          arg_id                                           argument  \\\n",
      "0        arg_0_0  `people reach their limit when it comes to the...   \n",
      "1        arg_0_1  A patient should be able to decide when they h...   \n",
      "2        arg_0_2  a person has the right to end their suffering ...   \n",
      "3        arg_0_3  a person should have the dignity to choose how...   \n",
      "4        arg_0_4  a person should have the right to be able to c...   \n",
      "...          ...                                                ...   \n",
      "5578  arg_27_218  we should subsidize vocational education to en...   \n",
      "5579  arg_27_219  We should subsidize vocational education to su...   \n",
      "5580  arg_27_220  While many who graduate from universities stru...   \n",
      "5581  arg_27_221  with the rising cost of college tuition vocati...   \n",
      "5582  arg_27_222  yes, we should subsidize vocational education ...   \n",
      "\n",
      "                                              topic  stance  \n",
      "0     Assisted suicide should be a criminal offence      -1  \n",
      "1     Assisted suicide should be a criminal offence      -1  \n",
      "2     Assisted suicide should be a criminal offence      -1  \n",
      "3     Assisted suicide should be a criminal offence      -1  \n",
      "4     Assisted suicide should be a criminal offence      -1  \n",
      "...                                             ...     ...  \n",
      "5578       We should subsidize vocational education       1  \n",
      "5579       We should subsidize vocational education       1  \n",
      "5580       We should subsidize vocational education       1  \n",
      "5581       We should subsidize vocational education       1  \n",
      "5582       We should subsidize vocational education       1  \n",
      "\n",
      "[5583 rows x 4 columns]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\n",
      "    key_point_id                                          key_point  \\\n",
      "0         kp_0_0  Assisted suicide gives dignity to the person t...   \n",
      "1         kp_0_1                 Assisted suicide reduces suffering   \n",
      "2         kp_0_2  People should have the freedom to choose to en...   \n",
      "3         kp_0_3  The terminally ill would benefit from assisted...   \n",
      "4         kp_0_4  Assisted suicide allows people to solicit some...   \n",
      "..           ...                                                ...   \n",
      "202      kp_27_3      subsidizing vocational education is expensive   \n",
      "203      kp_27_4  subsidizing vocational education promotes thos...   \n",
      "204      kp_27_5       vocational education is a good career choice   \n",
      "205      kp_27_6     vocational education better fits many students   \n",
      "206      kp_27_7  vocational education is beneficial for the ent...   \n",
      "\n",
      "                                             topic  stance  \n",
      "0    Assisted suicide should be a criminal offence      -1  \n",
      "1    Assisted suicide should be a criminal offence      -1  \n",
      "2    Assisted suicide should be a criminal offence      -1  \n",
      "3    Assisted suicide should be a criminal offence      -1  \n",
      "4    Assisted suicide should be a criminal offence       1  \n",
      "..                                             ...     ...  \n",
      "202       We should subsidize vocational education      -1  \n",
      "203       We should subsidize vocational education       1  \n",
      "204       We should subsidize vocational education       1  \n",
      "205       We should subsidize vocational education       1  \n",
      "206       We should subsidize vocational education       1  \n",
      "\n",
      "[207 rows x 4 columns]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\n",
      "           arg_id key_point_id  label\n",
      "0         arg_0_0       kp_0_0      0\n",
      "1       arg_0_121       kp_0_4      0\n",
      "2       arg_0_121       kp_0_5      0\n",
      "3       arg_0_121       kp_0_6      1\n",
      "4       arg_0_121       kp_0_7      0\n",
      "...           ...          ...    ...\n",
      "20630  arg_27_221      kp_27_6      0\n",
      "20631  arg_27_221      kp_27_7      0\n",
      "20632  arg_27_222      kp_27_4      0\n",
      "20633  arg_27_222      kp_27_5      1\n",
      "20634  arg_27_222      kp_27_7      0\n",
      "\n",
      "[20635 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arguments_df, key_points_df, labels_file_df = load_kpm_data(\"kpm_data\", \"train\")\n",
    "print(\"°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\")\n",
    "print (arguments_df)\n",
    "print(\"°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\")\n",
    "print(key_points_df)\n",
    "print(\"°°°°°°°°°°°°°°°°°°°°°°°°°°°°°à\")\n",
    "print(labels_file_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argDictArray = []\n",
    "keyDictArray = []\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "porter = stemmer()\n",
    "lastId = 0\n",
    "argDictArray.append({})\n",
    "for index, row in arguments_df.iterrows(): \n",
    "    _,idSet,id  = row[0].split(\"_\")\n",
    "    \n",
    "    if(idSet!=lastId):\n",
    "        argDictArray.append({})\n",
    "        lastId = int(idSet)\n",
    "\n",
    "    \n",
    "    filtered_words = row[1].lower() #to lower case\n",
    "    filtered_words = \"\".join([char for char in filtered_words if char not in string.punctuation]) # remove punctuation \n",
    "    filtered_words = tokenizer(filtered_words, \"english\")   # tokenize\n",
    "    filtered_words = [word for word in filtered_words if word not in stop_words] # remove stopwords (try without removing during training)\n",
    "    filtered_words = [porter.stem(word) for word in filtered_words] #stemming\n",
    "    print(row[1])\n",
    "    print(filtered_words)\n",
    "    argDictArray[lastId][id] = filtered_words\n",
    "    #print(argDictArray[lastId][id]) \n",
    "    \n",
    "    #TODO add synonyms and antonyms \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastId = 0\n",
    "keyDictArray.append({})\n",
    "for index, row in key_points_df.iterrows(): \n",
    "    _,idSet,id  = row[0].split(\"_\")\n",
    "    \n",
    "    if(idSet!=lastId):\n",
    "        keyDictArray.append({})\n",
    "        lastId = int(idSet)\n",
    "\n",
    "    \n",
    "    filtered_words = row[1].lower() #to lower case\n",
    "    filtered_words = \"\".join([char for char in filtered_words if char not in string.punctuation]) # remove punctuation \n",
    "    filtered_words = tokenizer(filtered_words, \"english\")   # tokenize\n",
    "    filtered_words = [word for word in filtered_words if word not in stop_words] # remove stopwords (try without removing during training)\n",
    "    filtered_words = [porter.stem(word) for word in filtered_words] #stemming\n",
    "    #print(row[1])\n",
    "    #print(filtered_words)\n",
    "    keyDictArray[lastId][id] = filtered_words\n",
    "    #print(keyDictArray[lastId][id]) \n",
    "    \n",
    "#TODO add synonyms and antonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"   \n",
    "os.environ['WANDB_NAME'] = 'KPM_HLT_2022'\n",
    "os.environ['WANDB_API_KEY'] = '20eb6383f49b2e6f666de5b53b5db5ece12bb3a1'\n",
    "#os.environ['WANDB_MODE']='offline'\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "import wandb\n",
    "\n",
    "def wandb_save_predicted(self, input_value, target_value, cv):\n",
    "        predicted = []\n",
    "        data = [] \n",
    "        counter = 0\n",
    "        for index, x in enumerate(input_value):\n",
    "            predicted.append(self.__model.predict(x))\n",
    "        \n",
    "        for index, x in enumerate(predicted):\n",
    "            for j in range(len(x)):\n",
    "                data.append([np.abs(x[j] - target_value[index][j]), index*len(x)+j])\n",
    "                wandb.log({\n",
    "                    \"Predicted \"+str(cv): x[j],\n",
    "                    \"Target \"+str(cv): target_value[index][j]\n",
    "                })\n",
    "        table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n",
    "        wandb.log({\n",
    "            \"plot\" : wandb.plot.line(table, \"x\", \"y\")\n",
    "        })\n",
    " \n",
    "        \n",
    "wandb.init(\n",
    "            #Set entity to specify your username or team name\n",
    "            entity=\"Paolo\",\n",
    "            #Set the project where this run will be logged            \n",
    "            project=\"nlp_test_0\",\n",
    "            group=\"experiment_\",\n",
    "    \t    #Track hyperparameters and run metadata\n",
    "            reinit=True)\n",
    "\n",
    "wandb.finish()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models from TensorFlow Hub, we will pick just a few for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bf7ec7796c28fcceddad23e71914adfb2c336f69250a3fccce6ef4cf4a8bfcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
